{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1dVc4Jr_8At"
      },
      "source": [
        "# Prepara√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzZuZRLO_-vo"
      },
      "source": [
        "## Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmtRgtKB_1w8"
      },
      "outputs": [],
      "source": [
        "# Extra√ß√£o\n",
        "!pip -q install --upgrade py7zr wget unidecode\n",
        "\n",
        "# Google CLoud: BigQuery e GCS\n",
        "!pip install --upgrade -q google-cloud-bigquery google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "05RsF1g_AHuP"
      },
      "outputs": [],
      "source": [
        "# Google Colab e servi√ßos de Cloud\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "\n",
        "# Bibliotecas padr√£o\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import shutil\n",
        "import tempfile\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import Optional, List\n",
        "\n",
        "# Manipula√ß√£o de dados\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Extra√ß√£o\n",
        "import requests\n",
        "import wget\n",
        "import ftplib\n",
        "from py7zr import SevenZipFile\n",
        "from bs4 import BeautifulSoup\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRhBXg5MAQQP"
      },
      "source": [
        "## Configurando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vq--YRX7APz_"
      },
      "outputs": [],
      "source": [
        "# Autenticar\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mKtSSj8AVFe"
      },
      "outputs": [],
      "source": [
        "# Inicializa o cliente do GCS\n",
        "PROJECT_ID = \"\"\n",
        "BUCKET_NAME = ''\n",
        "PATH_GCS = ''\n",
        "\n",
        "gcs_client = storage.Client(project=PROJECT_ID)\n",
        "bucket = gcs_client.get_bucket(BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S_hHKbgOWTwN"
      },
      "outputs": [],
      "source": [
        "# Outros par√¢metros\n",
        "ftp_base = 'ftp://ftp.mtps.gov.br/pdet/microdados/NOVO CAGED/'\n",
        "\n",
        "TMP_DIR = '/tmp/colab_caged'\n",
        "os.makedirs(TMP_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DTFf-DpQX3nw"
      },
      "outputs": [],
      "source": [
        "# Logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "logger = logging.getLogger(\"caged_etl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbgnL4kLLuZt"
      },
      "source": [
        "## Fun√ß√µes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MRWf1Sj3Lu6U"
      },
      "outputs": [],
      "source": [
        "def carregar_dados_dieese() -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Executa o web scraping da p√°gina do DIEESE para obter a s√©rie hist√≥rica\n",
        "    do Sal√°rio M√≠nimo Nominal e do Sal√°rio M√≠nimo Necess√°rio.\n",
        "    Retorna um DataFrame ou None em caso de falha.\n",
        "\n",
        "    A Metodologia do c√°lculo do SM Necess√°rio pode ser consultada em:\n",
        "    https://www.dieese.org.br/metodologia/metodologiaCestaBasica2016.pdf\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.info(\"üöå Extraindo dados de refer√™ncia do DIEESE...\")\n",
        "\n",
        "        # Visualizando html da p√°gina\n",
        "        url = \"https://www.dieese.org.br/analisecestabasica/salarioMinimo.html\"\n",
        "        response = requests.get(url, timeout=15)\n",
        "        pagina = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Criando a tabela atrav√©s dos elementos na p√°gina\n",
        "        linhas = pagina.find_all('tr')\n",
        "        dados = []\n",
        "        ano_atual = None\n",
        "\n",
        "        for linha in linhas:\n",
        "            # Verifica se √© um ano (class=\"subtitulo\")\n",
        "            if \"subtitulo\" in linha.get(\"class\", []):\n",
        "                ano_tag = linha.find(\"a\")\n",
        "                if ano_tag:\n",
        "                    ano_atual = ano_tag.get_text().strip()\n",
        "            # Verifica se √© uma linha de dados (exatamente 3 colunas (td))\n",
        "            elif len(linha.find_all(\"td\")) == 3 and ano_atual is not None:\n",
        "                colunas = linha.find_all(\"td\")\n",
        "                mes = colunas[0].get_text(strip=True).replace(\"\\n\", \"\")\n",
        "                salario_nominal = colunas[1].get_text(strip=True).replace(\"\\n\", \"\")\n",
        "                salario_necessario = colunas[2].get_text(strip=True).replace(\"\\n\", \"\")\n",
        "\n",
        "\n",
        "                dados.append({\n",
        "                    \"Ano\": ano_atual, \"M√™s\": mes,\n",
        "                    \"Sal√°rio M√≠nimo Nominal\": salario_nominal,\n",
        "                    \"Sal√°rio M√≠nimo Necess√°rio\": salario_necessario\n",
        "                })\n",
        "\n",
        "        if not dados:\n",
        "            logger.warning(\"Nenhum dado extra√≠do do site do DIEESE.\")\n",
        "            return None\n",
        "\n",
        "        sm_dieese = pd.DataFrame(dados)\n",
        "\n",
        "        # Convertendo informa√ß√£o de data\n",
        "        meses_para_numeros = {\n",
        "            'Janeiro': '01', 'Fevereiro': '02', 'Mar√ßo': '03', 'Abril': '04',\n",
        "            'Maio': '05', 'Junho': '06', 'Julho': '07', 'Agosto': '08',\n",
        "            'Setembro': '09', 'Outubro': '10', 'Novembro': '11', 'Dezembro': '12'\n",
        "        }\n",
        "\n",
        "        sm_dieese['Data'] = sm_dieese['Ano'].astype(str) + '-' + sm_dieese['M√™s']\n",
        "        sm_dieese['Data'] = sm_dieese['Data'].replace(meses_para_numeros, regex=True)\n",
        "        sm_dieese['Data'] = pd.to_datetime(sm_dieese['Data'] + '-01', format='%Y-%m-%d')\n",
        "        sm_dieese.drop(columns=['Ano', 'M√™s'], inplace=True)\n",
        "\n",
        "        # Limpeza das colunas monet√°rias\n",
        "        for col in ['Sal√°rio M√≠nimo Nominal', 'Sal√°rio M√≠nimo Necess√°rio']:\n",
        "            temp_col = (sm_dieese[col]\n",
        "                        .str.strip()\n",
        "                        .str.replace(\"R\\$\", \"\", regex=True)\n",
        "                        .str.replace(r\"\\.\", \"\", regex=False)\n",
        "                        .str.replace(\",\", \".\", regex=False)\n",
        "                        )\n",
        "            sm_dieese[col] = pd.to_numeric(temp_col, errors='coerce')\n",
        "\n",
        "        # Renomeia as colunas para facilitar o merge\n",
        "        sm_dieese.rename(columns={\n",
        "            'Data': 'competenciamov',\n",
        "            'Sal√°rio M√≠nimo Nominal': 'salario_minimo_nominal',\n",
        "            'Sal√°rio M√≠nimo Necess√°rio': 'salario_minimo_necessario'\n",
        "        }, inplace=True)\n",
        "\n",
        "        logger.info(\"‚úÖ Dados do DIEESE extra√≠dos e formatados com sucesso.\")\n",
        "        return sm_dieese[['competenciamov', 'salario_minimo_nominal', 'salario_minimo_necessario']]\n",
        "    except requests.RequestException as e:\n",
        "        logger.error(f\"Falha na requisi√ß√£o ao site do DIEESE: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro inesperado ao processar dados do DIEESE: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WsiA8wLpewuo"
      },
      "outputs": [],
      "source": [
        "def transformar_caged(df: pd.DataFrame, tipo_arquivo: str, df_dieese: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Aplica transforma√ß√µes iniciais, convers√µes de tipo e enriquecimento no DataFrame do CAGED.\n",
        "\n",
        "  Par√¢metros:\n",
        "        df (pd.DataFrame): DataFrame bruto do Novo CAGED, j√° lido a partir do TXT.\n",
        "        tipo_arquivo (str): Tipo do arquivo ('CAGEDMOV', 'CAGEDFOR' ou 'CAGEDEXC').\n",
        "        df_dieese (pd.DataFrame, opcional): DataFrame com informa√ß√µes mensais do DIEESE\n",
        "            (sal√°rio m√≠nimo nominal e necess√°rio). Pode ser None.\n",
        "\n",
        "    Retorna:\n",
        "        pd.DataFrame: DataFrame transformado e com colunas relevantes padronizadas.\n",
        "  \"\"\"\n",
        "  logger.info(f\"Iniciando transforma√ß√£o de {len(df)} registros...\")\n",
        "\n",
        "  # 1. Padroniza nomes das colunas para evitar problemas de chave (min√∫sculas, sem acentos e caracteres especiais)\n",
        "  df.columns = [unidecode(col).lower().replace(' ', '_').replace('.', '') for col in df.columns]\n",
        "\n",
        "  # 2. Converte tipos para colunas-chave e num√©ricas\n",
        "  df['saldomovimentacao'] = pd.to_numeric(df['saldomovimentacao'], errors='coerce')\n",
        "  df['competenciamov'] = pd.to_datetime(df['competenciamov'].astype(str) + '01', format='%Y%m%d', errors='coerce')\n",
        "  df['salario'] = df['salario'].str.replace(',', '.', regex=False)\n",
        "  df['salario'] = pd.to_numeric(df['salario'], errors='coerce').fillna(0)\n",
        "  df['idade'] = pd.to_numeric(df['idade'], errors='coerce')\n",
        "  df['regiao'] = pd.to_numeric(df['regiao'], errors='coerce')\n",
        "\n",
        "  # 3. Ajusta sinal de admiss√µes e desligamentos no caso de arquivos de exclus√£o (CAGEDEXC)\n",
        "  #    - No CAGEDEXC, movimenta√ß√µes s√£o exclu√≠das, ent√£o o sinal √© invertido.\n",
        "  if tipo_arquivo == \"CAGEDEXC\":\n",
        "      df['admissoes'] = np.where(df['saldomovimentacao'] == 1, -1, 0)\n",
        "      df['desligamentos'] = np.where(df['saldomovimentacao'] == -1, -1, 0)\n",
        "      df['saldomovimentacao'] = np.where(df['saldomovimentacao'] == 1, -1, 1)\n",
        "  else:\n",
        "      df['admissoes'] = np.where(df['saldomovimentacao'] == 1, 1, 0)\n",
        "      df['desligamentos'] = np.where(df['saldomovimentacao'] == -1, 1, 0)\n",
        "\n",
        "  # 4. Integra dados do DIEESE (sal√°rio m√≠nimo nominal e necess√°rio) por compet√™ncia\n",
        "  if df_dieese is not None:\n",
        "      df = df.merge(df_dieese, on='competenciamov', how='left')\n",
        "  else:\n",
        "      logger.warning(\"DataFrame do DIEESE n√£o fornecido. Faixas salariais n√£o ser√£o calculadas.\")\n",
        "      df['salario_minimo_nominal'] = np.nan\n",
        "      df['salario_minimo_necessario'] = np.nan\n",
        "\n",
        "  # 5. Cria faixas salariais com base no sal√°rio m√≠nimo nominal do DIEESE\n",
        "  sm_nominal = df['salario_minimo_nominal']\n",
        "  condicoes = [\n",
        "      (df['salario'] > 0) & (df['salario'] <= 1 * sm_nominal),\n",
        "       (df['salario'] > 1 * sm_nominal) & (df['salario'] <= 2 * sm_nominal),\n",
        "        (df['salario'] > 2 * sm_nominal) & (df['salario'] <= 3 * sm_nominal),\n",
        "         (df['salario'] > 3 * sm_nominal) & (df['salario'] <= 4 * sm_nominal),\n",
        "          (df['salario'] > 4 * sm_nominal) & (df['salario'] <= 5 * sm_nominal),\n",
        "           (df['salario'] > 5 * sm_nominal) & (df['salario'] <= 10 * sm_nominal)\n",
        "  ]\n",
        "  opcoes = ['At√© 1 SM', '1 a 2 SM', '2 a 3 SM', '3 a 4 SM', '4 a 5 SM', '5 a 10 SM']\n",
        "  df['faixa_salarial'] = np.select(condicoes, opcoes, default='Acima de 10 SM')\n",
        "  df.loc[(df['salario'] == 0) | (sm_nominal.isna()), 'faixa_salarial'] = 'N√£o Informado'\n",
        "\n",
        "  # 6. Cria faixas et√°rias a partir da idade (pd.cut facilita o binning)\n",
        "  bins_idade = [9, 14, 17, 24, 29, 39, 49, 64, float('inf')]\n",
        "  labels_idade = ['10-14 anos', '15-17 anos', '18-24 anos', '25-29 anos', '30-39 anos', '40-49 anos', '50-64 anos', '65+ anos']\n",
        "  df['faixa_etaria'] = pd.cut(df['idade'], bins=bins_idade, labels=labels_idade, right=True)\n",
        "\n",
        "  # 7. Seleciona e ordena colunas relevantes para o output\n",
        "  colunas_finais = [\n",
        "      'competenciamov', 'uf', 'municipio', 'secao', 'subclasse', 'cbo2002ocupacao',\n",
        "      'graudeinstrucao', 'idade', 'faixa_etaria', 'horascontratuais', 'racacor', 'sexo', 'salario',\n",
        "      'faixa_salarial', 'salario_minimo_necessario', 'admissoes', 'desligamentos', 'saldomovimentacao'\n",
        "  ]\n",
        "  colunas_existentes = [col for col in colunas_finais if col in df.columns]\n",
        "\n",
        "  logger.info(\"‚úÖ Transforma√ß√£o conclu√≠da.\")\n",
        "  return df[colunas_existentes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ONqXujwnHJSQ"
      },
      "outputs": [],
      "source": [
        "def pipeline_etl(ano: int, mes: int, df_dieese: Optional[pd.DataFrame], tipo_especifico: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    Executa o pipeline ETL para os microdados do Novo CAGED, do download √† carga no Google Cloud Storage.\n",
        "\n",
        "    O Novo CAGED inicia em janeiro de 2020, e esta fun√ß√£o n√£o processa per√≠odos anteriores devido a diferen√ßas metodol√≥gicas definidas pelo MTE.\n",
        "\n",
        "    Tipos de arquivo processados:\n",
        "        - CAGEDMOV: Movimenta√ß√µes declaradas dentro do prazo (compet√™ncia igual a AAAAMM).\n",
        "        - CAGEDFOR: Movimenta√ß√µes declaradas fora do prazo (compet√™ncia igual a AAAAMM).\n",
        "        - CAGEDEXC: Movimenta√ß√µes exclu√≠das (compet√™ncia de exclus√£o igual a AAAAMM).\n",
        "\n",
        "    Etapas do processo:\n",
        "        1. Baixa o arquivo .7z do FTP do MTE.\n",
        "        2. Extrai o .txt em mem√≥ria.\n",
        "        3. Aplica transforma√ß√µes e enriquecimento via `transformar_caged`.\n",
        "        4. Salva como Parquet e envia para o GCS.\n",
        "\n",
        "    Args:\n",
        "        ano (int): Ano do arquivo (>= 2020).\n",
        "        mes (int): M√™s do arquivo (1-12).\n",
        "        df_dieese (Optional[pd.DataFrame]): DataFrame com informa√ß√µes salariais do DIEESE para integra√ß√£o.\n",
        "        tipo_especifico (Optional[str], default=None): Um dos valores [\"CAGEDMOV\", \"CAGEDFOR\", \"CAGEDEXC\"].\n",
        "            Se None, processa todos.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Erros de conex√£o, leitura, ou transforma√ß√£o (com retentativas autom√°ticas).\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Configura√ß√µes do Pipeline\n",
        "    if ano < 2020:\n",
        "        logger.warning(f\"Ano {ano} √© anterior ao Novo CAGED. Ignorando...\")\n",
        "        return\n",
        "\n",
        "    tipos_padrao  = ['CAGEDEXC', 'CAGEDFOR', 'CAGEDMOV']\n",
        "    tipos_a_processar = [tipo_especifico] if tipo_especifico else tipos_padrao\n",
        "    mes_str = str(mes).zfill(2)\n",
        "\n",
        "    servidor_ftp = 'ftp.mtps.gov.br'\n",
        "    caminho_remoto_base = '/pdet/microdados/NOVO CAGED/'\n",
        "\n",
        "    # Para caso haja problema com a conex√£o com o servidor\n",
        "    numero_de_tentativas = 3\n",
        "\n",
        "    # In√≠cio do Processo com Diret√≥rio Tempor√°rio\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        for tipo in tipos_a_processar:\n",
        "            # Loop de retentativas para cada arquivo\n",
        "            for tentativa in range(numero_de_tentativas):\n",
        "                try:\n",
        "                    nome_arquivo_7z = f'{tipo}{ano}{mes_str}.7z'\n",
        "                    caminho_remoto = f'{caminho_remoto_base}{ano}/{ano}{mes_str}/'\n",
        "                    caminho_local_7z = os.path.join(tmpdir, nome_arquivo_7z)\n",
        "\n",
        "                    if tentativa == 0:\n",
        "                        start_time_str = datetime.now().strftime(\"%H:%M\")\n",
        "                        print(f\"[{start_time_str}] --- Iniciando pipeline para: {nome_arquivo_7z} ---\")\n",
        "\n",
        "\n",
        "                    # DOWNLOAD\n",
        "                    print(\"  [1/4] Baixando arquivo via FTP (pode demorar)...\")\n",
        "                    with ftplib.FTP(servidor_ftp, timeout=90) as ftp:\n",
        "                        ftp.login() # Login an√¥nimo\n",
        "                        ftp.cwd(caminho_remoto)\n",
        "                        with open(caminho_local_7z, 'wb') as f:\n",
        "                            ftp.retrbinary(f'RETR {nome_arquivo_7z}', f.write)\n",
        "                    print(\"  ‚úÖ Download conclu√≠do.\")\n",
        "\n",
        "                    # VERIFICA√á√ÉO DE INTEGRIDADE\n",
        "                    tamanho_bytes = os.path.getsize(caminho_local_7z)\n",
        "                    with open(caminho_local_7z, 'rb') as f:\n",
        "                        primeiros_bytes = f.read(6)\n",
        "                    if tamanho_bytes < 1000 or primeiros_bytes != b\"7z\\xbc\\xaf'\\x1c\":\n",
        "                        print(f\"  ‚ùå Arquivo baixado √© inv√°lido ou corrompido. Pulando.\")\n",
        "                        continue\n",
        "\n",
        "                    # EXTRA√á√ÉO\n",
        "                    print(\"  [2/4] Extraindo arquivo .txt...\")\n",
        "                    with SevenZipFile(caminho_local_7z, mode='r') as z:\n",
        "                        z.extractall(path=tmpdir)\n",
        "                    print(\"  ‚úÖ Extra√ß√£o conclu√≠da.\")\n",
        "\n",
        "                    # LEITURA PARA DATAFRAME\n",
        "                    caminho_local_txt = os.path.join(tmpdir, f\"{tipo}{ano}{mes_str}.txt\")\n",
        "                    df_raw = pd.read_csv(\n",
        "                        caminho_local_txt,\n",
        "                        sep=';',\n",
        "                        dtype=str,\n",
        "                        low_memory=False,\n",
        "                        encoding='utf-8'\n",
        "                    )\n",
        "\n",
        "                    # TRANSFORMA√á√ÉO\n",
        "                    print(f\"  [3/4] Transformando {len(df_raw)} registros...\")\n",
        "                    df_processed = transformar_caged(df_raw, tipo, df_dieese)\n",
        "                    print(\"  ‚úÖ Transforma√ß√£o conclu√≠da.\")\n",
        "\n",
        "                    # CARGA PARA O CLOUD STORAGE\n",
        "                    print(\"  [4/4] Enviando arquivo Parquet para o Cloud Storage...\")\n",
        "                    path_gcs_completo = f\"{PATH_GCS}/ano={ano}/mes={mes_str}/{tipo}{ano}{mes_str}.parquet\"\n",
        "                    blob = bucket.blob(path_gcs_completo)\n",
        "                    blob.upload_from_string(df_processed.to_parquet(index=False))\n",
        "                    end_time_str = datetime.now().strftime(\"%H:%M\")\n",
        "                    print(f\"[{end_time_str}]   ‚úÖ Carga conclu√≠da com sucesso para: {path_gcs_completo}\")\n",
        "\n",
        "                    break\n",
        "\n",
        "                except ftplib.error_perm:\n",
        "                    print(f\"  ‚ùå Arquivo n√£o encontrado no servidor. N√£o haver√° novas tentativas.\")\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ö†Ô∏è Tentativa {tentativa + 1} de {numero_de_tentativas} falhou: {e}\")\n",
        "                    if tentativa + 1 < numero_de_tentativas:\n",
        "                            tempo_espera = 15 * (2 ** tentativa)\n",
        "                            print(f\"  -> Aguardando {tempo_espera} segundos para tentar novamente...\")\n",
        "                            time.sleep(tempo_espera)\n",
        "                    else:\n",
        "                            logger.error(f\"  ‚ùå Todas as {numero_de_tentativas} tentativas falharam para {nome_arquivo_7z}. Desistindo.\")\n",
        "\n",
        "            print(\"  -> Pausando por 10 segundos para n√£o sobrecarregar o servidor...\")\n",
        "            time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xknbViYQAcsO"
      },
      "source": [
        "# Execu√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4M2jcr0FcPm"
      },
      "source": [
        "## Completa (2020 - Hoje)\n",
        "\n",
        "Na data em que a extra√ß√£o foi realizada, os arquivos na origem passavam por atualiza√ß√µes - isso acarretou em pastas vazias ou com arquivos corrompidos, gerando os problemas listados abaixo.\n",
        "\n",
        "| Data       | Per√≠odo    | Arquivo(s)         | Log                              | Status                                      | Pr√≥xima A√ß√£o                           |\n",
        "| ---------- | ---------- | ------------------ | -------------------------------- | ------------------------------------------- | -------------------------------------- |\n",
        "| 2025-08-12 | 2020/01    | CAGEDEXC, CAGEDFOR | ‚ùå N√£o encontrado no servidor     | ‚úÖ Comportamento esperado. Nenhuma a√ß√£o.     | ‚Äî                                      |\n",
        "| 2025-08-12 | 2020/02 | CAGEDEXC           | ‚ùå N√£o encontrado no servidor     | ‚úÖ Comportamento esperado. Nenhuma a√ß√£o.     | ‚Äî                                      |\n",
        "| 2025-08-12 | 2020/03 | CAGEDEXC           | ‚ùå N√£o encontrado no servidor     | ‚úÖ Comportamento esperado. Nenhuma a√ß√£o.     | ‚Äî                                      |\n",
        "| 2025-08-12 | 2020/03    | CAGEDFOR           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2020/07    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2020/12    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2021/04    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2021/05    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2021/06    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2021/07    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2021/08    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2021/09    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2021/10    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2021/11    | CAGEDFOR, CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2021/12    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2022/01    | CAGEDFOR           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2022/02    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2022/03    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2022/04    | CAGEDFOR, CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2022/06    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2022/07    | CAGEDMOV           | ‚ùå Arquivo inv√°lido ou corrompido | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2025/05    | CAGEDEXC, CAGEDFOR, CAGEDMOV           | ‚ùå A pasta de arquivos na origem estava vazia na data de extra√ß√£o | ‚ö†Ô∏è Pipeline pulou arquivo e seguiu execu√ß√£o | Monitorar e notificar MTE se persistir |\n",
        "| 2025-08-12 | 2025/07    | CAGEDEXC, CAGEDFOR, CAGEDMOV           | ‚ùå N√£o encontrado no servidor     | ‚úÖ Comportamento esperado. Nenhuma a√ß√£o.     | ‚Äî                                      |\n",
        "| 2025-08-12 | 2025/08    | CAGEDEXC, CAGEDFOR, CAGEDMOV           | ‚ùå N√£o encontrado no servidor     | ‚úÖ Comportamento esperado. Nenhuma a√ß√£o.     | ‚Äî                                      |\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oPDH2lnsFiS1"
      },
      "outputs": [],
      "source": [
        "# 1. Carrega a tabela de refer√™ncia do DIEESE uma √∫nica vez para ser usada no loop.\n",
        "tabela_referencia_dieese = carregar_dados_dieese()\n",
        "\n",
        "if tabela_referencia_dieese is not None:\n",
        "\n",
        "    # 2. Define o ano e m√™s atuais para saber onde parar.\n",
        "    ano_atual = datetime.now().year\n",
        "    mes_atual = datetime.now().month\n",
        "\n",
        "    logger.info(f\"O ETL ser√° executado de 01/2020 at√© {mes_atual:02d}/{ano_atual}.\")\n",
        "\n",
        "    # 3. Loop pelos anos, de 2020 at√© o ano atual.\n",
        "    for ano in range(2020, ano_atual + 1):\n",
        "\n",
        "        # Define at√© que m√™s o loop interno deve ir.\n",
        "        limite_mes = 12 if ano < ano_atual else mes_atual\n",
        "\n",
        "        for mes in range(1, limite_mes + 1):\n",
        "            pipeline_etl(\n",
        "                ano=ano,\n",
        "                mes=mes,\n",
        "                df_dieese=tabela_referencia_dieese\n",
        "            )\n",
        "else:\n",
        "    logger.error(\"ETL n√£o executado: falha ao carregar dados de refer√™ncia do DIEESE.\")\n",
        "\n",
        "logger.info(\"üèÅ Pipeline de execu√ß√£o completa finalizado. üèÅ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i29_DmMAFjY2"
      },
      "source": [
        "## Precisa\n",
        "Reprocessamento de arquivos que falharam devido a problemas de conex√£o/corrup√ß√£o ou caso deseje extrair arquivos de um per√≠odo espec√≠fico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHKY3Es_FlV2"
      },
      "outputs": [],
      "source": [
        "# 1. Lista de arquivos\n",
        "arquivos_para_reprocessar = [\n",
        "    \"CAGEDFOR202003.7z\", \"CAGEDMOV202007.7z\", \"CAGEDMOV202012.7z\",\n",
        "    \"CAGEDMOV202104.7z\", \"CAGEDMOV202105.7z\", \"CAGEDMOV202106.7z\",\n",
        "    \"CAGEDMOV202107.7z\", \"CAGEDMOV202108.7z\", \"CAGEDMOV202109.7z\",\n",
        "    \"CAGEDMOV202110.7z\", \"CAGEDFOR202111.7z\", \"CAGEDMOV202111.7z\",\n",
        "    \"CAGEDMOV202112.7z\", \"CAGEDFOR202201.7z\", \"CAGEDMOV202202.7z\",\n",
        "    \"CAGEDMOV202203.7z\", \"CAGEDFOR202204.7z\", \"CAGEDMOV202204.7z\",\n",
        "    \"CAGEDMOV202206.7z\", \"CAGEDMOV202207.7z\"\n",
        "]\n",
        "\n",
        "# 2. Carrega a tabela de refer√™ncia do DIEESE, se necess√°rio\n",
        "if 'tabela_referencia_dieese' not in locals() or tabela_referencia_dieese is None:\n",
        "    tabela_referencia_dieese = carregar_dados_dieese()\n",
        "\n",
        "if tabela_referencia_dieese is not None:\n",
        "    logger.info(f\"Ser√£o reprocessados {len(arquivos_para_reprocessar)} arquivos.\")\n",
        "\n",
        "    # 3. Loop atrav√©s da lista de arquivos para reprocessar\n",
        "    for nome_arquivo in arquivos_para_reprocessar:\n",
        "\n",
        "        # Extrai o tipo, ano e m√™s do nome do arquivo\n",
        "        tipo = nome_arquivo[:8]\n",
        "        ano = int(nome_arquivo[8:12])\n",
        "        mes = int(nome_arquivo[12:14])\n",
        "\n",
        "        # Chama o pipeline passando o tipo_especifico\n",
        "        pipeline_etl(\n",
        "            ano=ano,\n",
        "            mes=mes,\n",
        "            df_dieese=tabela_referencia_dieese,\n",
        "            tipo_especifico=tipo\n",
        "        )\n",
        "else:\n",
        "    logger.error(\"Reprocessamento n√£o executado: falha ao carregar dados de refer√™ncia do DIEESE.\")\n",
        "\n",
        "logger.info(\"üèÅ Reprocessamento de falhas finalizado. üèÅ\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
