{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPAqpoPjM9Gk"
   },
   "source": [
    "# Preparação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Izy_c8lGM-l",
    "outputId": "8c90aa31-3f39-45bb-a820-b9c260e9b110"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# !pip install pandas pyarrow\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# !pip install chardet\n",
    "import chardet\n",
    "import chardet.universaldetector\n",
    "\n",
    "# !pip install unidecode\n",
    "import unidecode\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "import os\n",
    "from os import remove\n",
    "import gc\n",
    "\n",
    "# !pip install wget\n",
    "import wget\n",
    "\n",
    "# !pip install py7zr\n",
    "from py7zr import SevenZipFile\n",
    "\n",
    "import glob\n",
    "\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diretório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U1fUcfCAGwXh"
   },
   "outputs": [],
   "source": [
    "# Ajustando diretório de trabalho\n",
    "pwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEXom9FiNCmy"
   },
   "source": [
    "# Download e Extração dos arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6SfSDnvNKBac"
   },
   "outputs": [],
   "source": [
    "def extract_caged (ano, mes):\n",
    "  \"\"\" Faz o download dos arquivos do NOVO CAGED para o ano e o mês especificados.\n",
    "  O período do NOVO CAGED tem início em Janeiro de 2020.\n",
    "  Nesse script não extraímos os microdados antriores a esse período devido a divergências metodológicas estabelecidas pelo MTE.\n",
    "\n",
    "  São 3 tipos de arquivo: CAGEDMOV, CAGEDFOR e CAGEDEXC.\n",
    "    1. CAGEDMOV: movimentações declaradas dentro do prazo com competência de declaração igual a AAAAMM.\n",
    "    2. CAGEDFOR: movimentações declaradas fora do prazo com competência de declaração igual a AAAAMM.\n",
    "    3. CAGEDEXC: movimentações excluídas com competência de declaração da exclusão igual a AAAAMM.\n",
    "  \n",
    "  Argumentos:\n",
    "      ano: Ano do arquivo.\n",
    "      mes: Mês do arquivo.\n",
    "  \"\"\"\n",
    "  # Download\n",
    "  if ano < 2020:\n",
    "        print(f\"Os microdados de {mes}/{ano} não serão baixados.\")\n",
    "        return\n",
    "\n",
    "  tipos_arquivos = ['CAGEDMOV', 'CAGEDEXC', 'CAGEDFOR']\n",
    "\n",
    "  for tipo in tipos_arquivos:\n",
    "    url = f'ftp://ftp.mtps.gov.br/pdet/microdados/NOVO CAGED/{ano}/{ano}{mes}/{tipo}{ano}{mes}.7z'\n",
    "    nome_arquivo = f'{tipo}{ano}{mes}.7z'\n",
    "\n",
    "    try:\n",
    "      print(f'\\nBaixando {tipo} de {mes}/{ano}...')\n",
    "      wget.download(url, nome_arquivo)\n",
    "      print(f'\\n{tipo}{ano}{mes}.7z baixado com sucesso.')\n",
    "    except:\n",
    "      print(f'\\nOs microdados {tipo} de {mes}/{ano} não estão disponíveis.')\n",
    "      continue\n",
    "\n",
    "    # Extração do arquivo 7z\n",
    "    try:\n",
    "        with SevenZipFile(nome_arquivo, mode = 'r') as archive:\n",
    "             archive.extractall()\n",
    "             print(f'{tipo}{ano}{mes}.txt extraído com sucesso.')\n",
    "    except:\n",
    "        print(f'Erro ao extrair {tipo}{ano}{mes}.7z.')\n",
    "        continue\n",
    "\n",
    "    # Remoção do arquivo 7z após a extração\n",
    "    try:\n",
    "        remove(nome_arquivo)\n",
    "        print(f'{nome_arquivo} removido.\\n')\n",
    "    except:\n",
    "        print(f'Erro ao remover {nome_arquivo}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções para detectar e remover linhas defeituosas dos dados brutos\n",
    "\n",
    "# Listando arquivos para corrigir\n",
    "arquivos_com_erro = [\n",
    "    \"dados_brutos/CAGEDFOR202008.txt\",\n",
    "    \"dados_brutos/CAGEDFOR202211.txt\",\n",
    "    \"dados_brutos/CAGEDFOR202212.txt\",\n",
    "    \"dados_brutos/CAGEDMOV202302.txt\"\n",
    "]\n",
    "\n",
    "# Mapeamento das colunas antigas para as novas\n",
    "mapa_colunas = {\n",
    "    'compet√™nciamov': 'competênciamov',\n",
    "    'regi√£o': 'região',\n",
    "    'uf': 'uf',\n",
    "    'munic√≠pio': 'município',\n",
    "    'se√ß√£o': 'seção',\n",
    "    'subclasse': 'subclasse',\n",
    "    'saldomovimenta√ß√£o': 'saldomovimentação',\n",
    "    'cbo2002ocupa√ß√£o': 'cbo2002ocupação',\n",
    "    'categoria': 'categoria',\n",
    "    'graudeinstru√ß√£o': 'graudeinstrução',\n",
    "    'idade': 'idade',\n",
    "    'horascontratuais': 'horascontratuais',\n",
    "    'ra√ßacor': 'raçacor',\n",
    "    'sexo': 'sexo',\n",
    "    'tipoempregador': 'tipoempregador',\n",
    "    'tipoestabelecimento': 'tipoestabelecimento',\n",
    "    'tipomovimenta√ß√£o': 'tipomovimentação',\n",
    "    'tipodedefici√™ncia': 'tipodedeficiência',\n",
    "    'indtrabintermitente': 'indtrabintermitente',\n",
    "    'indtrabparcial': 'indtrabparcial',\n",
    "    'sal√°rio': 'salário',\n",
    "    'tamestabjan': 'tamestabjan',\n",
    "    'indicadoraprendiz': 'indicadoraprendiz',\n",
    "    'origemdainforma√ß√£o': 'origemdainformação',\n",
    "    'compet√™nciadec': 'competênciadec',\n",
    "    'indicadordeforadoprazo': 'indicadordeforadoprazo',\n",
    "    'unidadesal√°rioc√≥digo': 'unidadesaláriocódigo',\n",
    "    'valorsal√°riofixo': 'valorsaláriofixo'\n",
    "}\n",
    "\n",
    "delimitadores_esperados = 27 # Número esperado de delimitadores para 28 colunas\n",
    "\n",
    "def remover_linhas_defeituosas(arquivo, linhas_para_remover_conteudo=None, encoding_entrada='macroman', encoding_saida='utf-8'):\n",
    "    linhas_validas = []\n",
    "\n",
    "    with open(arquivo, 'r', encoding=encoding_entrada) as f:\n",
    "        for i, linha in enumerate(f):\n",
    "            # Remover a linha específica pelo índice, caso esteja na lista de linhas para remover\n",
    "            if linhas_para_remover_conteudo and any (conteudo in linha for conteudo in linhas_para_remover_conteudo):\n",
    "                continue  # Pula a linha\n",
    "            # Adiciona a linha se tiver o número correto de delimitadores\n",
    "            if linha.count(';') == delimitadores_esperados:\n",
    "                linhas_validas.append(linha)\n",
    "\n",
    "    # Salvando o arquivo corrigido no formato correto (utf-8)\n",
    "    novo_arquivo = arquivo.replace(\".txt\", \"_corrigido.txt\")\n",
    "   \n",
    "    with open(novo_arquivo, 'w', encoding=encoding_saida) as f:\n",
    "        f.writelines(linhas_validas)\n",
    "\n",
    "    print(f\"Arquivo corrigido salvo como: {novo_arquivo}\")\n",
    "    return novo_arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar coluna de faixa etária\n",
    "def adicionar_faixaeta(idade):\n",
    "    if idade >= 10 and idade <=14:\n",
    "        fxet = 1\n",
    "    elif idade >= 15 and idade <=17:\n",
    "        fxet = 2\n",
    "    elif idade >= 18 and idade <=24:\n",
    "        fxet = 3\n",
    "    elif idade >= 25 and idade <=29:\n",
    "        fxet = 4\n",
    "    elif idade >= 30 and idade <=39:\n",
    "        fxet = 5\n",
    "    elif idade >= 40 and idade <=49:\n",
    "        fxet = 6\n",
    "    elif idade >= 50 and idade < 64:\n",
    "        fxet = 7\n",
    "    elif idade > 65:\n",
    "        fxet = 8\n",
    "    else:\n",
    "        fxet = 99\n",
    "    return fxet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para converter arquivos corrigidos para parquet\n",
    "def converter_txt_to_parquet(input_dir, output_dir, padrao, sep=';', encoding='latin1', compression='snappy'):\n",
    "    \"\"\"\n",
    "    Converte arquivos .txt em um diretório para o formato Parquet, salvando os resultados em outro diretório.\n",
    "    \n",
    "    Argumentos:\n",
    "        input_dir (str): Diretório onde estão os arquivos .txt.\n",
    "        output_dir (str): Diretório onde os arquivos .parquet serão salvos.\n",
    "        padrao (str): Padrão de nomeação dos arquivos a serem convertidos (e.g., 'CAGED').\n",
    "        sep (str): Separador dos arquivos .txt. Padrão é ';'.\n",
    "        encoding (str): Codificação dos arquivos .txt. Padrão é 'latin1'.\n",
    "        compression (str): Tipo de compressão para os arquivos Parquet. Padrão é 'snappy'.\n",
    "    \"\"\"\n",
    "    # Certificar-se de que o diretório de saída existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Percorrer todos os arquivos no diretório de entrada\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        # Verificar se o arquivo segue o padrão especificado e é .txt\n",
    "        if padrao in file_name and file_name.endswith('.txt'):\n",
    "            input_file = os.path.join(input_dir, file_name)\n",
    "            output_file = os.path.join(output_dir, file_name.replace('.txt', '.parquet'))\n",
    "\n",
    "            try:\n",
    "                print(f'Convertendo {file_name} para Parquet...')\n",
    "                # Ler o arquivo .txt\n",
    "                df = pd.read_csv(input_file, sep=sep, encoding=encoding)\n",
    "                # Salvar como Parquet\n",
    "                df.to_parquet(output_file, engine='pyarrow', compression=compression)\n",
    "                print(f'{file_name} convertido para {output_file} com sucesso.')\n",
    "            except Exception as e:\n",
    "                print(f'Erro ao converter {file_name}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbovT9RSNOYa"
   },
   "outputs": [],
   "source": [
    "# Extraindo arquivos para todo o período\n",
    "\n",
    "# Definir meses de janeiro a dezembro\n",
    "meses = [f'{i:02}' for i in range(1, 13)]\n",
    "\n",
    "# Loop\n",
    "for ano in range(2020, 2025):\n",
    "    for mes in meses:\n",
    "        extract_caged(ano, mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uwffAxQdsMO",
    "outputId": "43ec350b-2e8b-4a2f-a53b-d31e992a3a22"
   },
   "outputs": [],
   "source": [
    "# Extraindo arquivos de um mês específico\n",
    "extract_caged(2024, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo Dicionários\n",
    "dicionarios = pd.read_excel(pwd + \"\\\\arquivos auxiliares\\\\dicionários.xlsx\",\n",
    "                           sheet_name=None)\n",
    "\n",
    "dic_classes = dicionarios[list(dicionarios.keys())[0]]\n",
    "dic_faixaeta = dicionarios[list(dicionarios.keys())[1]]\n",
    "dic_escolaridade = dicionarios[list(dicionarios.keys())[2]]\n",
    "dic_sexo = dicionarios[list(dicionarios.keys())[3]]\n",
    "dic_racacor = dicionarios[list(dicionarios.keys())[4]]\n",
    "dic_local = dicionarios[list(dicionarios.keys())[5]]\n",
    "\n",
    "\n",
    "dicionarios_2 = pd.read_excel(pwd + \"\\\\arquivos auxiliares\\\\Layout Novo Caged Movimentação.xlsx\",\n",
    "                              sheet_name=None)\n",
    "\n",
    "dic_regiao = dicionarios_2[list(dicionarios_2.keys())[1]]\n",
    "dic_muni = dicionarios_2[list(dicionarios_2.keys())[3]]\n",
    "dic_categoria = dicionarios_2[list(dicionarios_2.keys())[7]]\n",
    "dic_cbo2002 = dicionarios_2[list(dicionarios_2.keys())[8]]\n",
    "dic_empregador = dicionarios_2[list(dicionarios_2.keys())[11]]\n",
    "dic_estab = dicionarios_2[list(dicionarios_2.keys())[12]]\n",
    "dic_movimentacao = dicionarios_2[list(dicionarios_2.keys())[13]]\n",
    "dic_trab_parcial = dicionarios_2[list(dicionarios_2.keys())[14]]\n",
    "dic_trab_intermitente = dicionarios_2[list(dicionarios_2.keys())[15]]\n",
    "dic_deficiencia = dicionarios_2[list(dicionarios_2.keys())[16]]\n",
    "dic_aprendiz = dicionarios_2[list(dicionarios_2.keys())[17]]\n",
    "dic_tamaestab = dicionarios_2[list(dicionarios_2.keys())[18]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando mapas\n",
    "mapa_ufs = dict(zip(dic_local['UF'], dic_local['Nome_UF']))\n",
    "mapa_fxeta = dict(zip(dic_faixaeta['cod'], dic_faixaeta['nom']))\n",
    "mapa_escolaridade = dict(zip(dic_escolaridade['cod'], dic_escolaridade['nom']))\n",
    "mapa_sexo = dict(zip(dic_sexo['cod'], dic_sexo['nom']))\n",
    "mapa_racacor = dict(zip(dic_racacor['cod'], dic_racacor['nom']))\n",
    "mapa_secao = dict(zip(dic_classes['Seção'], dic_classes['Nome Seção']))\n",
    "mapa_classe = dict(zip(dic_classes['Classe'], dic_classes['Nome Classe']))\n",
    "mapa_subclasse = dict(zip(dic_classes['Subclasse'], dic_classes['Nome Subclasse']))\n",
    "\n",
    "\n",
    "mapa_muni = dict(zip(dic_muni['Código'], dic_muni['Descrição']))\n",
    "mapa_regiao = dict(zip(dic_regiao['Código'], dic_regiao['Descrição']))\n",
    "mapa_categoria = dict(zip(dic_categoria['Código'], dic_categoria['Descrição']))\n",
    "mapa_cbo2002 = dict(zip(dic_cbo2002['Código'], dic_cbo2002['Descrição']))\n",
    "mapa_empregador = dict(zip(dic_empregador['Código'], dic_empregador['Descrição']))\n",
    "mapa_estab = dict(zip(dic_estab['Código'], dic_estab['Descrição']))\n",
    "mapa_movimentacao = dict(zip(dic_movimentacao['Código'], dic_movimentacao['Descrição']))\n",
    "mapa_trab_parcial = dict(zip(dic_trab_parcial['Código'], dic_trab_parcial['Descrição']))\n",
    "mapa_trab_intermitente = dict(zip(dic_trab_intermitente['Código'], dic_trab_intermitente['Descrição']))\n",
    "mapa_deficiencia = dict(zip(dic_deficiencia['Código'], dic_deficiencia['Descrição']))\n",
    "mapa_aprendiz = dict(zip(dic_aprendiz['Código'], dic_aprendiz['Descrição']))\n",
    "mapa_tamaestab = dict(zip(dic_tamaestab['Código'], dic_tamaestab['Descrição']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrigindo arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processando arquivos com defeitos\n",
    "for arquivo in arquivos_com_erro:\n",
    "    try:\n",
    "        print(f\"\\nProcessando o arquivo: {arquivo}\")\n",
    "\n",
    "        # Lista com índices de linhas a serem removidas\n",
    "        linhas_defeituosas_conteudo = [\"18,03\", \"G\"] #CAGEDFOR202008\n",
    "\n",
    "        # Remover as linhas defeituosas e salvar o arquivo corrigido\n",
    "        arquivo_corrigido = remover_linhas_defeituosas(arquivo, linhas_para_remover_conteudo=linhas_defeituosas_conteudo, encoding_entrada='macroman', encoding_saida='utf-8')\n",
    "        \n",
    "        # Carregar e processar o arquivo corrigido\n",
    "        df = pd.read_csv(arquivo_corrigido, sep=\";\", encoding='utf-8')\n",
    "        df.rename(columns=mapa_colunas, inplace=True)\n",
    "        df.to_csv(arquivo_corrigido, sep=\";\", encoding='utf-8', index=False)\n",
    "\n",
    "        print(f\"Colunas renomeadas e arquivo salvo: {arquivo_corrigido}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o arquivo {arquivo}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertendo para Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter_txt_to_parquet(\n",
    "    input_dir=pwd + \"\\\\dados_brutos\\\\\",\n",
    "    output_dir=pwd + \"\\\\dados_processados\\\\\",\n",
    "    padrao='CAGED',\n",
    "    sep=';',\n",
    "    encoding='latin1',\n",
    "    compression='snappy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório para salvar os arquivos processados\n",
    "diretorio_processados = pwd + \"\\\\dados_processados\\\\\"\n",
    "os.makedirs(diretorio_processados, exist_ok=True)\n",
    "\n",
    "# Arquivo de cache\n",
    "cache_file = os.path.join(diretorio_processados, \"processamento_cache.csv\")\n",
    "\n",
    "# Criar arquivo de cache, se não existir\n",
    "if not os.path.exists(cache_file):\n",
    "    cache = pd.DataFrame(columns=[\"arquivo\", \"status\"])\n",
    "    cache.to_csv(cache_file, index=False)\n",
    "else:\n",
    "    cache = pd.read_csv(cache_file)\n",
    "\n",
    "# Verificar se o arquivo já foi processado\n",
    "def verificar_cache(arquivo):\n",
    "    if arquivo in cache[\"arquivo\"].values:\n",
    "        status = cache.loc[cache[\"arquivo\"] == arquivo, \"status\"].values[0]\n",
    "        return status\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Atualizar o cache com o status do arquivo\n",
    "def atualizar_cache(arquivo, status):\n",
    "    global cache\n",
    "    if arquivo in cache[\"arquivo\"].values:\n",
    "        cache.loc[cache[\"arquivo\"] == arquivo, \"status\"] = status\n",
    "    else:\n",
    "        cache = pd.concat([cache, pd.DataFrame({\"arquivo\": [arquivo], \"status\": [status]})], ignore_index=True)\n",
    "    cache.to_csv(cache_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processando arquivos\n",
    "\n",
    "tipos_arquivos = ['CAGEDEXC', 'CAGEDFOR', 'CAGEDMOV2020', 'CAGEDMOV2021', 'CAGEDMOV2022', 'CAGEDMOV2023', 'CAGEDMOV2024']\n",
    "diretorio_processados = pwd + \"\\\\dados_processados\\\\\"\n",
    "\n",
    "# Colunas necessárias\n",
    "colunas_necessarias = ['competênciamov', 'região', 'uf', 'município', \n",
    "                       'seção', 'subclasse', 'cbo2002ocupação', 'categoria',\n",
    "                       'graudeinstrução', 'idade', 'raçacor', 'sexo', 'tipodedeficiência',\n",
    "                       'indtrabintermitente', 'indtrabparcial', 'indicadoraprendiz', \n",
    "                       'salário', 'valorsaláriofixo', 'unidadesaláriocódigo',\n",
    "                       'tipoempregador', 'tipoestabelecimento', 'tamestabjan',\n",
    "                       'tipomovimentação', 'saldomovimentação'\n",
    "                       ]\n",
    "\n",
    "\n",
    "mapeamento_colunas = {\n",
    "    'competÃªnciamov': 'competênciamov',\n",
    "    'regiÃ£o': 'região',\n",
    "    'municÃ­pio': 'município',\n",
    "    'seÃ§Ã£o': 'seção',\n",
    "    'saldomovimentaÃ§Ã£o': 'saldomovimentação',\n",
    "    'cbo2002ocupaÃ§Ã£o': 'cbo2002ocupação',\n",
    "    'graudeinstruÃ§Ã£o': 'graudeinstrução',\n",
    "    'raÃ§acor': 'raçacor',\n",
    "    'tipomovimentaÃ§Ã£o': 'tipomovimentação',\n",
    "    'tipodedeficiÃªncia': 'tipodedeficiência',\n",
    "    'salÃ¡rio': 'salário',\n",
    "    'unidadesalÃ¡riocÃ³digo': 'unidadesaláriocódigo',\n",
    "    'valorsalÃ¡riofixo': 'valorsaláriofixo',\n",
    "}\n",
    "\n",
    "for tipo in tipos_arquivos:\n",
    "\n",
    "    # Listando arquivos disponíveis\n",
    "    arquivos = glob.glob(f\"dados_processados\\\\{tipo}*.parquet\")\n",
    "    print(f\"\\nArquivos encontrados: {arquivos}\")\n",
    "\n",
    "    # Lista para armezanar arquivos processados\n",
    "    lista_arquivos = []\n",
    "\n",
    "    # Função para liberar memória\n",
    "    def liberar_memoria():\n",
    "        gc.collect()\n",
    "        print(\"Memória liberada\")\n",
    "\n",
    "    # Loop pelos arquivos encontrados\n",
    "    for arquivo in arquivos:\n",
    "    \n",
    "        # Verificar status no cache\n",
    "        status = verificar_cache(arquivo)\n",
    "\n",
    "        # Se o arquivo já foi processado, pular\n",
    "        if status == \"processado\":\n",
    "            print(f\"Arquivo {arquivo} já processado. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"\\nProcessando: {arquivo}\")\n",
    "            data = pd.read_parquet(arquivo)\n",
    "            data.rename(columns=mapeamento_colunas, inplace=True)\n",
    "\n",
    "            # Mapeamento de colunas\n",
    "            data['região'] = data['região'].map(mapa_regiao).fillna('Não Identificado')\n",
    "            data['uf'] = data['uf'].map(mapa_ufs).fillna('Não Identificado')\n",
    "            data['município'] = data['município'].astype(int)\n",
    "            data['município'] = data['município'].map(mapa_muni).fillna('Não Identificado')\n",
    "            data['município'] = data['município'].str.slice(start=3)\n",
    "            data['faixa_etaria'] = data['idade'].apply(adicionar_faixaeta)\n",
    "            data['faixa_etaria'] = data['faixa_etaria'].map(mapa_fxeta).fillna(\"Não Identificado\")\n",
    "            data['graudeinstrução'] = data['graudeinstrução'].map(mapa_escolaridade).fillna(\"Não Identificado\")\n",
    "            data['sexo'] = data['sexo'].map(mapa_sexo).fillna(\"Não Identificado\")\n",
    "            data['raçacor'] = data['raçacor'].map(mapa_racacor).fillna(\"Não Identificado\")\n",
    "            data['seção'] = data['seção'].map(mapa_secao).fillna(\"Não Identificado\")   \n",
    "            data['subclasse'] = data['subclasse'].map(mapa_subclasse).fillna(\"Não Identificado\") \n",
    "            data['categoria'] = data['categoria'].map(mapa_categoria).fillna(\"Não Identificado\") \n",
    "            data['cbo2002ocupação'] = data['cbo2002ocupação'].map(mapa_cbo2002).fillna(\"Não Identificado\")\n",
    "            data['tipoempregador'] = data['tipoempregador'].map(mapa_empregador).fillna(\"Não Identificado\")\n",
    "            data['tipoestabelecimento'] = data['tipoestabelecimento'].map(mapa_estab).fillna(\"Não Identificado\")\n",
    "            data['tipomovimentação'] = data['tipomovimentação'].map(mapa_movimentacao).fillna(\"Não Identificado\")\n",
    "            data['indtrabparcial'] = data['indtrabparcial'].map(mapa_trab_parcial).fillna(\"Não Identificado\")\n",
    "            data['indtrabintermitente'] = data['indtrabintermitente'].map(mapa_trab_intermitente).fillna(\"Não Identificado\")\n",
    "            data['tipodedeficiência'] = data['tipodedeficiência'].map(mapa_deficiencia).fillna(\"Não Identificado\")\n",
    "            data['indicadoraprendiz'] = data['indicadoraprendiz'].map(mapa_aprendiz).fillna(\"Não Identificado\")\n",
    "            data['tamestabjan'] = data['tamestabjan'].map(mapa_tamaestab).fillna(\"Não Identificado\")\n",
    "\n",
    "            # Adicionando coluna de admissões e desligamentos\n",
    "            if tipo == 'CAGEDEXC':\n",
    "                data['admissoes'] = data['saldomovimentação'].apply(lambda x: -1 if x == 1 else 0)\n",
    "                data['desligamentos'] = data['saldomovimentação'].apply(lambda x: -1 if x == -1 else 0)\n",
    "                data['saldomovimentação'] = data['saldomovimentação'].apply(lambda x: -1 if x == 1 else 1)\n",
    "            else:\n",
    "                data['admissoes'] = data['saldomovimentação'].apply(lambda x: 1 if x == 1 else 0)\n",
    "                data['desligamentos'] = data['saldomovimentação'].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "            # Transformação da coluna 'salário'\n",
    "            data['salário'] = data['salário'].apply(lambda x: str(x) if isinstance(x, (int, float)) else x)  # Converter valores numéricos para string\n",
    "            data['salário'] = pd.to_numeric(data['salário'].str.replace(',', '.').fillna('0'), errors='coerce')\n",
    "\n",
    "            # Agrupando e somando\n",
    "            data = data.groupby(data.columns.tolist(), as_index=False).agg({\n",
    "                'admissoes': 'sum',\n",
    "                'desligamentos': 'sum',\n",
    "                'saldomovimentação': 'sum'\n",
    "            })\n",
    "        \n",
    "            # Transformação para 'category' em colunas apropriadas para reduzir o uso de memória\n",
    "            colunas_category = ['região', 'uf', 'município', 'seção', 'subclasse', 'categoria', \n",
    "                                'graudeinstrução', 'sexo', 'raçacor', 'faixa_etaria', 'cbo2002ocupação', \n",
    "                                'tipodedeficiência', 'indtrabintermitente', 'indtrabparcial', 'indicadoraprendiz']\n",
    "        \n",
    "            for coluna in colunas_category:\n",
    "                data[coluna] = data[coluna].astype('category')\n",
    "        \n",
    "            # Adicionar df processado à lista\n",
    "            lista_arquivos.append(data)\n",
    "\n",
    "            # Atualizar cache após o processamento do arquivo\n",
    "            atualizar_cache(arquivo, \"processado\")\n",
    "\n",
    "            # Liberar memória após processamento de cada arquivo\n",
    "            liberar_memoria()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar o arquivo {arquivo}: {e}\")\n",
    "\n",
    "    # Concatenando todos os df\n",
    "    if lista_arquivos:\n",
    "        lista_arquivos = pd.concat(lista_arquivos, ignore_index=True)\n",
    "    else:\n",
    "        print(\"Nenhum arquivo foi processado.\")\n",
    "        continue\n",
    "\n",
    "    # Liberar memória após concatenar\n",
    "    liberar_memoria()\n",
    "\n",
    "    print(f\"{tipo} processada com sucesso!\")\n",
    "\n",
    "    # Salvando arquivo\n",
    "    nome_arquivo = f\"{tipo}.csv\"\n",
    "    caminho_arquivo = os.path.join(diretorio_processados, nome_arquivo)\n",
    "    lista_arquivos.to_csv(caminho_arquivo, index=False, encoding='latin1', sep=';')\n",
    "    print(f\"{nome_arquivo} salvo com sucesso!\")\n",
    "\n",
    "# Liberar memória após concatenar\n",
    "liberar_memoria()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
